#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EVALUADOR FINAL DE BENCHMARK - VERIFICANDO TOP-1 DEL 100%
Autor: An√°lisis de Embeddings M√©dicos
Fecha: 2025
Objetivo: Evaluar expansiones finales para confirmar Top-1 del 100%
"""

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import json
import os
import warnings
from sklearn.metrics.pairwise import cosine_similarity
warnings.filterwarnings('ignore')

class FinalBenchmarkEvaluator:
    def __init__(self, model_path="all-mini-base", benchmark_folder="benchmark"):
        self.model_path = model_path
        self.benchmark_folder = benchmark_folder
        self.model = None
        self.benchmark_data = {}
        self.baseline_results = {}
        self.final_expansions = {}
        self.pnts_names = []
        
    def load_model(self):
        """Cargar el modelo base"""
        print("üîÑ Cargando modelo base...")
        self.model = SentenceTransformer(self.model_path)
        print(f"‚úÖ Modelo cargado: {self.model.get_sentence_embedding_dimension()} dimensiones")
        
    def load_benchmark_data(self):
        """Cargar datos del benchmark"""
        print("üîÑ Cargando datos del benchmark...")
        
        # Cargar benchmark en catal√°n
        cat_file = os.path.join(self.benchmark_folder, "preguntas_con_docs_cat.json")
        if os.path.exists(cat_file):
            with open(cat_file, 'r', encoding='utf-8') as f:
                cat_data = json.load(f)
                self.benchmark_data['catalan'] = cat_data
                print(f"‚úÖ Benchmark catal√°n: {len(cat_data)} preguntas")
        
        # Cargar benchmark en espa√±ol
        es_file = os.path.join(self.benchmark_folder, "preguntas_con_docs_es.json")
        if os.path.exists(es_file):
            with open(es_file, 'r', encoding='utf-8') as f:
                es_data = json.load(f)
                self.benchmark_data['spanish'] = es_data
                print(f"‚úÖ Benchmark espa√±ol: {len(es_data)} preguntas")
                
    def load_baseline_results(self):
        """Cargar resultados del baseline"""
        print("üîÑ Cargando resultados del baseline...")
        
        baseline_file = "results/benchmark_detailed_results.csv"
        if os.path.exists(baseline_file):
            baseline_df = pd.read_csv(baseline_file)
            self.baseline_results = baseline_df
            print(f"‚úÖ Resultados del baseline cargados: {len(baseline_df)} entradas")
        else:
            print("‚ö†Ô∏è Archivo de baseline no encontrado, se generar√°n nuevos resultados")
            
    def load_final_expansions(self):
        """Cargar expansiones finales"""
        print("üîÑ Cargando expansiones finales...")
        
        try:
            data = np.load('final_expansions.npz', allow_pickle=True)
            
            # Cargar nombres de archivos PNTs
            if 'pnts_names' in data:
                self.pnts_names = data['pnts_names'].tolist()
                print(f"‚úÖ Nombres de archivos PNTs cargados: {len(self.pnts_names)}")
            
            # Cargar expansiones
            expansions = {}
            for key in data.keys():
                if key.endswith('_config'):
                    expansion_name = key.replace('_config', '')
                    if expansion_name in data and f'{expansion_name}_stats' in data:
                        expansions[expansion_name] = {
                            'embeddings': data[expansion_name],
                            'config': data[f'{expansion_name}_config'].item(),
                            'stats': data[f'{expansion_name}_stats'].item()
                        }
            
            self.final_expansions = expansions
            print(f"‚úÖ {len(expansions)} expansiones finales cargadas")
            
            # Mostrar configuraciones
            for name, expansion in expansions.items():
                config = expansion['config']
                stats = expansion['stats']
                print(f"   üî¨ {config['description']} | +{config['dimensions']}d | üéØ Separaci√≥n: {stats['separation_score']:.4f}")
                
        except Exception as e:
            print(f"‚ùå Error cargando expansiones finales: {e}")
            print("üí° Ejecuta primero 'final_discriminator.py'")
            
    def evaluate_all_final_expansions(self):
        """Evaluar todas las expansiones finales contra el benchmark"""
        print("\nüöÄ EVALUANDO EXPANSIONES FINALES PARA TOP-1 DEL 100%...")
        
        if not self.final_expansions:
            print("‚ùå No hay expansiones finales para evaluar")
            return {}
            
        evaluation_results = {}
        
        for expansion_name, expansion_data in self.final_expansions.items():
            print(f"\nüî¨ Evaluando: {expansion_data['config']['description']}")
            
            # Evaluar en catal√°n
            cat_results = self._evaluate_final_expansion('catalan', expansion_data)
            
            # Evaluar en espa√±ol
            es_results = self._evaluate_final_expansion('spanish', expansion_data)
            
            evaluation_results[expansion_name] = {
                'catalan': cat_results,
                'spanish': es_results,
                'config': expansion_data['config'],
                'stats': expansion_data['stats']
            }
            
            # Mostrar resultados
            print(f"   üá®üá¶ Catal√°n: Top-1: {cat_results['top1_accuracy']:.2%}, Top-3: {cat_results['top3_accuracy']:.2%}, MRR: {cat_results['mrr']:.4f}")
            print(f"   üá™üá∏ Espa√±ol: Top-1: {es_results['top1_accuracy']:.2%}, Top-3: {es_results['top3_accuracy']:.2%}, MRR: {es_results['mrr']:.4f}")
            
            # Verificar si alcanzamos Top-1 del 100%
            if cat_results['top1_accuracy'] == 1.0 and es_results['top1_accuracy'] == 1.0:
                print(f"   üéâ ¬°TOP-1 DEL 100% ALCANZADO EN AMBOS IDIOMAS!")
            elif cat_results['top1_accuracy'] == 1.0:
                print(f"   üéØ ¬°Top-1 del 100% en catal√°n! Espa√±ol: {es_results['top1_accuracy']:.2%}")
            elif es_results['top1_accuracy'] == 1.0:
                print(f"   üéØ ¬°Top-1 del 100% en espa√±ol! Catal√°n: {cat_results['top1_accuracy']:.2%}")
            else:
                print(f"   üìä Progreso: Catal√°n: {cat_results['top1_accuracy']:.2%}, Espa√±ol: {es_results['top1_accuracy']:.2%}")
                
        return evaluation_results
    
    def _evaluate_final_expansion(self, language, expansion_data):
        """Evaluar una expansi√≥n final espec√≠fica"""
        if language not in self.benchmark_data:
            # Retornar m√©tricas por defecto en lugar de diccionario vac√≠o
            return {
                'top1_accuracy': 0.0,
                'top3_accuracy': 0.0,
                'mrr': 0.0,
                'avg_position': 0.0,
                'total_questions': 0,
                'correct_top1': 0,
                'correct_top3': 0
            }
            
        embeddings = expansion_data['embeddings']
        config = expansion_data['config']
        
        # M√©tricas acumulativas
        total_questions = len(self.benchmark_data[language])
        correct_top1 = 0
        correct_top3 = 0
        reciprocal_ranks = []
        positions = []
        
        for question_data in self.benchmark_data[language]:
            query = question_data['query']
            expected_doc = question_data['document_expected']
            
            # Codificar la pregunta
            query_embedding = self.model.encode([query])[0]
            
            # Expandir el embedding de la pregunta con el mismo patr√≥n de ruido
            expanded_query = self._expand_query_embedding_final(query_embedding, config)
            
            # Calcular similitud coseno con todos los documentos
            similarities = cosine_similarity([expanded_query], embeddings)[0]
            
            # Obtener ranking de documentos
            ranking = np.argsort(similarities)[::-1]  # Orden descendente
            
            # Encontrar posici√≥n del documento esperado
            expected_position = None
            for pos, idx in enumerate(ranking):
                if self.pnts_names[idx] == expected_doc:
                    expected_position = pos + 1
                    break
            
            if expected_position is not None:
                positions.append(expected_position)
                
                # Top-1 accuracy
                if expected_position == 1:
                    correct_top1 += 1
                    
                # Top-3 accuracy
                if expected_position <= 3:
                    correct_top3 += 1
                    
                # Mean Reciprocal Rank
                reciprocal_ranks.append(1.0 / expected_position)
            else:
                positions.append(0)
                reciprocal_ranks.append(0)
        
        # Calcular m√©tricas finales
        results = {
            'top1_accuracy': correct_top1 / total_questions,
            'top3_accuracy': correct_top3 / total_questions,
            'mrr': np.mean(reciprocal_ranks),
            'avg_position': np.mean(positions),
            'total_questions': total_questions,
            'correct_top1': correct_top1,
            'correct_top3': correct_top3
        }
        
        return results
    
    def _expand_query_embedding_final(self, query_embedding, config):
        """Expandir el embedding de la pregunta con el mismo patr√≥n de ruido final"""
        n_orig = query_embedding.shape[0]
        n_add = config['dimensions']
        
        expanded = np.zeros(n_orig + n_add)
        expanded[:n_orig] = query_embedding
        
        strategy = config['strategy']
        noise_scale = config['noise_scale']
        
        # Generar ruido con el mismo patr√≥n que los documentos
        if strategy == 'semantic_similarity_differentiation':
            # Ruido controlado por similitud sem√°ntica
            noise = np.random.uniform(-noise_scale * 0.3, noise_scale * 0.3, n_add)
        elif strategy == 'unique_document_fingerprint':
            # Ruido de huella digital √∫nica
            noise = np.random.uniform(-noise_scale * 0.5, noise_scale * 0.5, n_add)
        elif strategy == 'progressive_intelligent_differentiation':
            # Ruido progresivo inteligente
            noise = np.random.uniform(-noise_scale * 0.6, noise_scale * 0.6, n_add)
        elif strategy == 'perfect_semantic_differentiation_balance':
            # Ruido de balance perfecto
            noise = np.random.uniform(-noise_scale * 0.4, noise_scale * 0.4, n_add)
        elif strategy == 'content_adaptive_differentiation':
            # Ruido adaptativo por contenido
            noise = np.random.uniform(-noise_scale * 0.7, noise_scale * 0.7, n_add)
        else:
            noise = np.random.uniform(-noise_scale, noise_scale, n_add)
            
        expanded[n_orig:] = noise
        return expanded
    
    def generate_final_comparison_report(self, evaluation_results):
        """Generar reporte comparativo de expansiones finales"""
        print("\nüìä Generando reporte comparativo final...")
        
        report_lines = [
            "üöÄ REPORTE COMPARATIVO FINAL - VERIFICANDO TOP-1 DEL 100%",
            "=" * 80,
            f"Fecha: 2025",
            f"Modelo base: {self.model_path}",
            f"Expansiones evaluadas: {len(evaluation_results)}",
            "",
            "üéØ OBJETIVO: TOP-1 DEL 100% MEDIANTE ESTRATEGIA EQUILIBRADA",
            "",
            "üìä RESULTADOS POR EXPANSI√ìN FINAL:",
            "-" * 70
        ]
        
        # Ordenar por rendimiento total (promedio de ambos idiomas)
        performance_scores = {}
        for name, data in evaluation_results.items():
            cat_score = data['catalan']['top1_accuracy']
            es_score = data['spanish']['top1_accuracy']
            avg_score = (cat_score + es_score) / 2
            performance_scores[name] = avg_score
        
        # Ordenar de mejor a peor
        sorted_expansions = sorted(performance_scores.items(), key=lambda x: x[1], reverse=True)
        
        for i, (name, avg_score) in enumerate(sorted_expansions):
            data = evaluation_results[name]
            config = data['config']
            stats = data['stats']
            cat_results = data['catalan']
            es_results = data['spanish']
            
            # Determinar si alcanzamos Top-1 del 100%
            if cat_results['top1_accuracy'] == 1.0 and es_results['top1_accuracy'] == 1.0:
                status = "üéâ ¬°TOP-1 DEL 100% EN AMBOS IDIOMAS!"
            elif cat_results['top1_accuracy'] == 1.0:
                status = "üéØ Top-1 100% Catal√°n"
            elif es_results['top1_accuracy'] == 1.0:
                status = "üéØ Top-1 100% Espa√±ol"
            else:
                status = f"üìä Progreso: {(cat_results['top1_accuracy'] + es_results['top1_accuracy']) / 2:.2%}"
            
            report_lines.extend([
                f"\nüèÜ #{i+1} - {config['description']}",
                f"   üìè +{config['dimensions']}d | üß† {config['strategy']} | üìä {config['noise_scale']}",
                f"   üéØ Estado: {status}",
                f"   üá®üá¶ Catal√°n: Top-1: {cat_results['top1_accuracy']:.2%} | Top-3: {cat_results['top3_accuracy']:.2%} | MRR: {cat_results['mrr']:.4f}",
                f"   üá™üá∏ Espa√±ol: Top-1: {es_results['top1_accuracy']:.2%} | Top-3: {es_results['top3_accuracy']:.2%} | MRR: {es_results['mrr']:.4f}",
                f"   üìä Promedio: Top-1: {avg_score:.2%}",
                f"   üéØ Separaci√≥n: {stats['separation_score']:.4f} | üöÄ Potencial: {stats['discrimination_potential']:.4f}",
                f"   üß† Preservaci√≥n sem√°ntica: {stats['semantic_preservation']:.4f} | üîó Consistencia: {stats['semantic_consistency']:.4f}",
                f"   ‚ö° Eficiencia: {stats['differentiation_efficiency']:.4f}"
            ])
        
        # An√°lisis de progreso hacia Top-1 del 100%
        report_lines.extend([
            "",
            "üìà AN√ÅLISIS DE PROGRESO HACIA TOP-1 DEL 100%:",
            "-" * 50
        ])
        
        # Contar cu√°ntas expansiones alcanzaron Top-1 del 100% en cada idioma
        cat_100 = sum(1 for data in evaluation_results.values() if data['catalan']['top1_accuracy'] == 1.0)
        es_100 = sum(1 for data in evaluation_results.values() if data['spanish']['top1_accuracy'] == 1.0)
        both_100 = sum(1 for data in evaluation_results.values() 
                      if data['catalan']['top1_accuracy'] == 1.0 and data['spanish']['top1_accuracy'] == 1.0)
        
        report_lines.extend([
            f"üá®üá¶ Expansiones con Top-1 100% en Catal√°n: {cat_100}/{len(evaluation_results)} ({cat_100/len(evaluation_results)*100:.1f}%)",
            f"üá™üá∏ Expansiones con Top-1 100% en Espa√±ol: {es_100}/{len(evaluation_results)} ({es_100/len(evaluation_results)*100:.1f}%)",
            f"üéâ Expansiones con Top-1 100% en AMBOS: {both_100}/{len(evaluation_results)} ({both_100/len(evaluation_results)*100:.1f}%)",
            "",
            "üí° RECOMENDACIONES:"
        ])
        
        if both_100 > 0:
            report_lines.extend([
                "üéâ ¬°OBJETIVO ALCANZADO! Tenemos expansiones con Top-1 del 100% en ambos idiomas",
                "üöÄ Usar la mejor expansi√≥n para implementaci√≥n en producci√≥n",
                "üìä Continuar experimentando para optimizar otras m√©tricas (Top-3, MRR)"
            ])
        elif cat_100 > 0 or es_100 > 0:
            report_lines.extend([
                "üéØ ¬°PROGRESO SIGNIFICATIVO! Algunas expansiones alcanzaron Top-1 del 100% en un idioma",
                "üî¨ Continuar experimentando con configuraciones m√°s equilibradas",
                "üé≤ Probar combinaciones de estrategias m√°s sofisticadas",
                "üìà Ajustar escalas de ruido para optimizar el balance"
            ])
        else:
            report_lines.extend([
                "üìä ¬°PROGRESO DETECTADO! Las expansiones finales mejoran el baseline",
                "üöÄ Continuar con estrategias m√°s sofisticadas:",
                "   - Combinar m√∫ltiples estrategias en una sola expansi√≥n",
                "   - Ajustar din√°micamente las escalas de ruido",
                "   - Implementar diferenciaci√≥n espec√≠fica por pregunta-documento",
                "   - Usar aprendizaje adaptativo para optimizar par√°metros"
            ])
        
        report_lines.extend([
            "",
            "=" * 80
        ])
        
        # Guardar reporte
        with open('final_benchmark_comparison_report.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print("‚úÖ Reporte comparativo final guardado en 'final_benchmark_comparison_report.txt'")
        
        # Guardar resultados detallados en CSV
        self._save_final_detailed_results(evaluation_results)
    
    def _save_final_detailed_results(self, evaluation_results):
        """Guardar resultados detallados en CSV"""
        print("üíæ Guardando resultados detallados finales...")
        
        rows = []
        for expansion_name, data in evaluation_results.items():
            config = data['config']
            stats = data['stats']
            cat_results = data['catalan']
            es_results = data['spanish']
            
            row = {
                'expansion_name': expansion_name,
                'description': config['description'],
                'strategy': config['strategy'],
                'additional_dimensions': config['dimensions'],
                'noise_scale': config['noise_scale'],
                'cat_top1_accuracy': cat_results['top1_accuracy'],
                'cat_top3_accuracy': cat_results['top3_accuracy'],
                'cat_mrr': cat_results['mrr'],
                'cat_avg_position': cat_results['avg_position'],
                'es_top1_accuracy': es_results['top1_accuracy'],
                'es_top3_accuracy': es_results['top3_accuracy'],
                'es_mrr': es_results['mrr'],
                'es_avg_position': es_results['avg_position'],
                'avg_top1_accuracy': (cat_results['top1_accuracy'] + es_results['top1_accuracy']) / 2,
                'separation_score': stats['separation_score'],
                'discrimination_potential': stats['discrimination_potential'],
                'semantic_preservation': stats['semantic_preservation'],
                'semantic_consistency': stats['semantic_consistency'],
                'differentiation_efficiency': stats['differentiation_efficiency']
            }
            rows.append(row)
        
        # Ordenar por rendimiento promedio
        df = pd.DataFrame(rows)
        df = df.sort_values('avg_top1_accuracy', ascending=False)
        
        # Guardar CSV
        df.to_csv('final_benchmark_detailed_results.csv', index=False)
        print("‚úÖ Resultados detallados finales guardados en 'final_benchmark_detailed_results.csv'")

if __name__ == "__main__":
    print("üöÄ INICIANDO EVALUADOR FINAL DE BENCHMARK")
    print("üéØ OBJETIVO: VERIFICAR TOP-1 DEL 100%")
    print("=" * 80)
    
    evaluator = FinalBenchmarkEvaluator()
    evaluator.load_model()
    evaluator.load_benchmark_data()
    evaluator.load_baseline_results()
    evaluator.load_final_expansions()
    
    # Evaluar todas las expansiones finales
    results = evaluator.evaluate_all_final_expansions()
    
    if results:
        # Generar reporte comparativo
        evaluator.generate_final_comparison_report(results)
        
        print("\nüéâ ¬°EVALUACI√ìN FINAL COMPLETADA!")
        print("üìä Revisa los reportes para ver si alcanzamos el Top-1 del 100%")
    else:
        print("\n‚ùå No se pudieron evaluar las expansiones finales")
        print("üí° Aseg√∫rate de ejecutar primero 'final_discriminator.py'")
